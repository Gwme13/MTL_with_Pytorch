{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNhqbKXboviV"
      },
      "source": [
        "# Didactic Exploration of Multi-Task vs. Single-Task Learning on CIFAR-10\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "In deep learning, we often train models to perform a single, specific task, this is **Single-Task Learning (STL)**. However, many real-world problems benefit from learning multiple related tasks simultaneously. This approach, known as **Multi-Task Learning (MTL)**, aims to improve a model's generalization capabilities and learning efficiency by leveraging shared knowledge across tasks. The underlying hypothesis is that learning related tasks together can act as a form of **inductive transfer** or regularization, leading to more robust representations.\n",
        "\n",
        "This notebook provides a hands-on, didactic comparison between STL and different MTL strategies. We will use the **CIFAR-10** dataset as our foundation and build models based on the classic **AlexNet** architecture. Our goal is not necessarily to achieve state-of-the-art results, but rather to understand the *mechanisms* and *behaviors* of these learning paradigms.\n",
        "\n",
        "### 1.1. Chosen Tasks\n",
        "\n",
        "We define two distinct tasks using the CIFAR-10 dataset:\n",
        "\n",
        "1.  **Task 1 (T1): Multi-class Object Classification**: The standard CIFAR-10 challenge: classifying an image into one of its 10 categories (e.g., airplane, dog, cat). This is our primary task.\n",
        "2.  **Task 2 (T2): Rotation Classification**: A custom task – determining if an input image has been rotated by 0°, 90°, 180°, or 270°. This task focuses more on orientation and low-level features.\n",
        "\n",
        "We *intentionally* chose these tasks because they are uncorrelated. Will learning about image orientation help in classifying objects, or will it interfere?\n",
        "\n",
        "### 1.2. Learning Approaches\n",
        "\n",
        "We will implement and compare three distinct learning approaches:\n",
        "\n",
        "1.  **Single-Task Learning (STL)**: Our baseline. We train two *completely independent* AlexNet models, one for each task.\n",
        "2.  **Hard Parameter Sharing MTL**:  We use *one* AlexNet model with a shared backbone and two *separate* classification heads. This forces tasks to share the same underlying representation.\n",
        "3.  **Soft Parameter Sharing MTL (Cross-Stitch Networks)**: A more flexible approach. We use *two* AlexNet models but insert \"Cross-Stitch\" units between certain layers. These units *learn* how to linearly combine features from both networks, allowing for an adaptive, 'soft' sharing of information.\n",
        "\n",
        "### 1.3. Learning Objectives\n",
        "\n",
        "By working through this notebook, we will:\n",
        "\n",
        "1. Understand the core concepts of Multi-Task Learning and how it differs from Single-Task Learning.\n",
        "2. Explore the benefits and challenges of Hard and Soft parameter sharing in MTL.\n",
        "3. Analyze the performance of STL vs. MTL approaches on CIFAR-10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgmBUmhBoviX",
        "outputId": "3e88030f-2793-4f2d-e72b-e01c83efba0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# ===================================================================\n",
        "# Setup: Import Libraries and Configure the Environment\n",
        "# ===================================================================\n",
        "# Here, we import all necessary libraries and configure our setup,\n",
        "# including setting up PyTorch to use a GPU if available and ensuring\n",
        "# reproducibility through random seeds.\n",
        "# ===================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mtick\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# -------------------------------------------\n",
        "# Configuration and Reproducibility\n",
        "# -------------------------------------------\n",
        "\n",
        "# Set a random seed for PyTorch, NumPy, and Python's random module.\n",
        "# This helps in making our experiments reproducible. While deep learning\n",
        "# training can have inherent randomness (especially on GPUs), setting\n",
        "# seeds makes it *more* likely to get similar results across runs.\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# -------------------------------------------\n",
        "# Device Configuration (GPU vs. CPU)\n",
        "# -------------------------------------------\n",
        "\n",
        "# We check if a CUDA-enabled GPU is available. If so, we set our 'device'\n",
        "# to 'cuda'; otherwise, we use the 'cpu'. Training deep learning models\n",
        "# is significantly faster on GPUs. The .to(device) method will be used\n",
        "# later to move our models and data to the chosen device.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# -------------------------------------------\n",
        "# Hyperparameters and Constants\n",
        "# -------------------------------------------\n",
        "\n",
        "# Define key hyperparameters for our training process.\n",
        "BATCH_SIZE = 128   # How many samples per batch to load.\n",
        "NUM_EPOCHS = 1   # Number of complete passes through the training dataset.\n",
        "LEARNING_RATE = 1e-3 # Controls how much we adjust model weights during training.\n",
        "NUM_CLASSES_T1 = 10 # CIFAR-10 has 10 classes (Task 1).\n",
        "NUM_CLASSES_T2 = 4  # Rotation task has 4 classes (0, 90, 180, 270 degrees).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FDAwa5ErlsD"
      },
      "source": [
        "## 2. Data Preparation: CIFAR-10 Dataset\n",
        "\n",
        "We need to load the CIFAR-10 dataset and prepare it for our two distinct tasks. This involves creating a custom PyTorch `Dataset` that can serve data for both tasks simultaneously.\n",
        "\n",
        "In PyTorch, the creation of custom datasets is easy. This is primarily accomplished by **extending the base `torch.utils.data.Dataset` class**. The core requirement is to override two essential methods:\n",
        "* `__len__(self)`: This method should return the total number of samples in the dataset.\n",
        "* `__getitem__(self, idx)`: This method is responsible for loading and returning a single sample from the dataset given an index `idx`.\n",
        "\n",
        "In PyTorch, typically, **class labels are represented as integer indices** (zero-based). This integer representation is the standard expectation for many of PyTorch's loss functions, such as `torch.nn.CrossEntropyLoss`. The standard CIFAR-10 dataset provides class labels in this format, where each class is represented by an integer from 0 to 9. Indeed, for the new task we will also use integer labels (0 for 0° rotation, 1 for 90°, etc.). This consistency allows us to use the standard classification loss functions without any additional conversion steps.\n",
        "\n",
        "### 2.1. MultiTaskCIFAR10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V4AoCadssMTX"
      },
      "outputs": [],
      "source": [
        "class MultiTaskCIFAR10(torchvision.datasets.CIFAR10):\n",
        "    \"\"\"\n",
        "    CIFAR-10 expanded for two tasks:\n",
        "      - Task 1: classify into 10 CIFAR-10 object classes\n",
        "      - Task 2: classify rotation into 4 angles (0°, 90°, 180°, 270°)\n",
        "\n",
        "    Each original image is repeated 4 times, once per rotation angle.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root, train=True, download=True, transform=None):\n",
        "        super().__init__(root, train=train, download=download,\n",
        "                         transform=None, target_transform=None)\n",
        "        self.transform = transform\n",
        "        # Define the rotation angles\n",
        "        self.angles = [0, 90, 180, 270]\n",
        "        # Expanded length = 4 × number of original images\n",
        "        # Tells PyTorch how many samples the new dataset has (4× more than CIFAR-10).\n",
        "        self.expanded_len = len(self.data) * len(self.angles)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the expanded dataset size\n",
        "        return self.expanded_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Picks one of the 50,000 original images.\n",
        "        angle_idx = idx % 4\n",
        "        # Picks one of the four rotations.\n",
        "        orig_idx  = idx // 4\n",
        "\n",
        "        # Load the original image as a PIL.Image\n",
        "        img = Image.fromarray(self.data[orig_idx])\n",
        "\n",
        "        # Get the original CIFAR-10 class label (0–9)\n",
        "        class_label = self.targets[orig_idx]\n",
        "\n",
        "        # Rotate the image without changing its size (32×32)\n",
        "        angle = self.angles[angle_idx]\n",
        "        img = img.rotate(angle, expand=False)\n",
        "\n",
        "        # Apply any final transforms (ToTensor, Normalize, etc.)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        # Rotation label is simply angle_idx (0..3)\n",
        "        rotation_label = angle_idx\n",
        "\n",
        "        # Return (image, class_label, rotation_label)\n",
        "        return img, class_label, rotation_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hKZapwHBaJf",
        "outputId": "718f8050-95cb-4756-a7c5-ce87ee87bb52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 30.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 200000\n",
            "Number of test samples: 40000\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------------------\n",
        "# Create Dataset Instances\n",
        "# -------------------------------------------\n",
        "# We create instances of our custom dataset for both training and validation.\n",
        "train_dataset = MultiTaskCIFAR10(root='./data/', train=True, download=True)\n",
        "test_dataset = MultiTaskCIFAR10(root='./data/', train=False, download=True)\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of test samples: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wkV3ok9BdrA",
        "outputId": "fbb7e36b-db8b-436a-d0de-a33d70e9c53d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: [0.49139968 0.48215841 0.44653091], Std.:[0.24703223 0.24348513 0.26158784]\n"
          ]
        }
      ],
      "source": [
        "mean = train_dataset.data.mean(axis=(0, 1, 2))/255\n",
        "std = train_dataset.data.std(axis=(0, 1, 2))/255\n",
        "\n",
        "print(f'Mean: {mean}, Std.:{std}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYj_br-Ztuim"
      },
      "source": [
        "**Note**: `ToTensor()` converts a PIL image or NumPy array into a float `torch.Tensor` with channels-first ordering and scales pixel values to [0, 1]. We then use `Normalize()` to adjust these tensor values based on the dataset's mean and standard deviation. This entire transformation process makes the data compatible with PyTorch models and helps optimize training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ua0DoO2MAGbe"
      },
      "outputs": [],
      "source": [
        "# Define the transformation pipeline including ToTensor and Normalize\n",
        "transform_pipeline = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Converts image to Tensor and scales to [0, 1]\n",
        "    transforms.Normalize(\n",
        "        mean=mean,  # CIFAR-10 Mean\n",
        "        std=std    # CIFAR-10 Std. Dev.\n",
        "    )\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIM6HNQUDvqO"
      },
      "source": [
        "**Important Note:** The mean and standard deviation values used for `Normalize` must **always** be calculated using **only the training dataset**. This is crucial to prevent *data leakage* – where information from the test or validation sets inadvertently influences the training process – and ensures a fair and realistic evaluation of the model's performance on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBVr-1mutq3Z",
        "outputId": "7da42f6d-d3a9-4dac-f34c-18fd905e1560"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 200000\n",
            "Number of tes samples: 40000\n"
          ]
        }
      ],
      "source": [
        "# -------------------------------------------\n",
        "# Create Dataset Instances\n",
        "# -------------------------------------------\n",
        "# We create instances of our custom dataset for both training and validation.\n",
        "train_dataset = MultiTaskCIFAR10(root='./data/', train=True, transform=transform_pipeline, download=True)\n",
        "test_dataset = MultiTaskCIFAR10(root='./data/', train=False, transform=transform_pipeline, download=True)\n",
        "\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of tes samples: {len(test_dataset)}\")\n",
        "\n",
        "# -------------------------------------------\n",
        "# Create DataLoaders\n",
        "# -------------------------------------------\n",
        "# DataLoaders are PyTorch utilities that provide an iterable over a dataset.\n",
        "# They handle batching, shuffling, and can use multiple worker processes\n",
        "# to load data in parallel, which is crucial for efficient GPU utilization.\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,  # Shuffling the data is important for training to prevent model bias and improve generalization.\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False, # No need to shuffle the validation/test set.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqQTFnmTttLV"
      },
      "source": [
        "### 2.2 Validation Set\n",
        "We will use a 10% of our training dataset as a validation set. This set will be used to evaluate the model's performance during training, allowing us to monitor for overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yFm-MG7EuQgh"
      },
      "outputs": [],
      "source": [
        "# 90% for training, 10% for validation\n",
        "val_size   = int(0.1 * len(train_dataset))\n",
        "train_size = len(train_dataset) - val_size\n",
        "\n",
        "# Split with a fixed seed for reproducibility\n",
        "train_dataset, val_dataset = random_split(\n",
        "    train_dataset,\n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(SEED)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X10GQd9YuhrM"
      },
      "source": [
        "### 2.3 Visualize a Sample Batch\n",
        "Before moving on, let's visualize a sample batch from our dataset to ensure everything is set up correctly. This will help us confirm that our custom dataset works as expected and that the images are correctly transformed and labeled for both tasks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "oZFop2VhvCfL",
        "outputId": "1e9bcf91-76fa-4e9b-fe93-bbb5c922c8f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Visualize a sample batch...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x600 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# -------------------------------------------\n",
        "# Visualize a Sample Batch\n",
        "# -------------------------------------------\n",
        "# We visualize a few samples to ensure\n",
        "# the data loading and transformations are working as expected.\n",
        "\n",
        "def imshow(img):\n",
        "    \"\"\"Helper function to display an image.\"\"\"\n",
        "    npimg = img.numpy() # Covert the image to Numpy array\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))  # from [C, H, W] to [H, W, C]\n",
        "    plt.axis('off')\n",
        "\n",
        "print(\"\\nVisualize a sample batch...\")\n",
        "\n",
        "# Get one batch of training images.\n",
        "dataiter = iter(train_loader)\n",
        "images, labels_t1, labels_t2 = next(dataiter)\n",
        "\n",
        "# CIFAR-10 class labels\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "# Task 2 labesl\n",
        "rotation_labels = ['0°', '90°', '180°', '270°']\n",
        "\n",
        "\n",
        "def imshow_denorm(img, mean, std):\n",
        "    \"\"\"Helper function to de-normalize and display an image.\"\"\"\n",
        "\n",
        "    # Ensure std and mean are in [C, 1, 1] shape for broadcasting\n",
        "    std = torch.tensor(std).view(3, 1, 1)\n",
        "    mean = torch.tensor(mean).view(3, 1, 1)\n",
        "\n",
        "    # De-normalize: img * std + mean\n",
        "    img = img * std + mean\n",
        "\n",
        "    # Clip to [0, 1] to handle potential floating point inaccuracies\n",
        "    # and ensure it's a valid range for imshow\n",
        "    img = torch.clamp(img, 0, 1)\n",
        "\n",
        "    # Convert to NumPy and transpose\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0))) # from [C, H, W] to [H, W, C]\n",
        "    plt.axis('off')\n",
        "\n",
        "\n",
        "# Plot each image in a 2x4 grid with corresponding labels\n",
        "# We will show just 8 samples\n",
        "plt.figure(figsize=(14, 6))\n",
        "for idx in range(len(images[:8])):\n",
        "    plt.subplot(2, 4, idx+1)\n",
        "    imshow_denorm(images[idx], mean, std)\n",
        "    title = f\"{class_names[labels_t1[idx]]}\\nRotation: {rotation_labels[labels_t2[idx]]}\"\n",
        "    plt.title(title, fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpK7SiJ2J8DH"
      },
      "source": [
        "## 3. Architectures\n",
        "To define the architectures we will use, which are based on **AlexNet**, we need:\n",
        "1.  A base `AlexNet` class.\n",
        "2.  A `HardMTL` class implementing hard parameter sharing.\n",
        "3.  A `CrossStitchUnit` and `CrossStitchNetwork` class for soft parameter sharing.\n",
        "4.  A `SingleTask` class for single-task learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN40IvIoO7YA"
      },
      "source": [
        "### 3.1. Base AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qEuVxdJaKfZX"
      },
      "outputs": [],
      "source": [
        "class AlexNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the AlexNet architecture, adapted for CIFAR-10.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_outputs=10):\n",
        "        \"\"\"\n",
        "        Initializes the AlexNet model.\n",
        "\n",
        "        Args:\n",
        "            num_outputs (int): The number of output neurons in the final layer.\n",
        "                               This will depend on the task (10 for T1, 4 for T2).\n",
        "        \"\"\"\n",
        "        super(AlexNet, self).__init__() # Always call the parent constructor.\n",
        "\n",
        "        # --- Feature Extractor (Convolutional Layers) ---\n",
        "        # This part extracts features from the input image.\n",
        "        self.features = nn.Sequential(\n",
        "            # Layer 1: Conv -> ReLU -> MaxPool\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1), # CIFAR-10 is small (32x32), so we adapt AlexNet.\n",
        "            nn.ReLU(inplace=True), # ReLU (Rectified Linear Unit) is a common activation function. 'inplace=True' saves memory.\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), # Max-pooling reduces spatial dimensions, providing some translation invariance.\n",
        "\n",
        "            # Layer 2: Conv -> ReLU -> MaxPool\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "            # Layer 3: Conv -> ReLU\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Layer 4: Conv -> ReLU\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Layer 5: Conv -> ReLU -> MaxPool\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        # --- Classifier (Fully-Connected Layers) ---\n",
        "        # This part takes the extracted features and performs classification.\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5), # Dropout is a regularization technique to prevent overfitting. It randomly sets some activations to 0.\n",
        "            nn.Linear(256 * 4 * 4, 1024), # Linear (fully-connected) layer. Input size = channels * height * width.\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(1024, num_outputs), # The final output layer, size depends on the task.\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the network.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input tensor (batch of images).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output tensor (logits for each class).\n",
        "        \"\"\"\n",
        "        # Pass through feature extractor.\n",
        "        x = self.features(x)\n",
        "        # Flatten the output for the classifier.\n",
        "        # torch.flatten(x, 1) flattens all dimensions except the batch dimension (dim 0).\n",
        "        x = torch.flatten(x, 1)\n",
        "        # Pass through classifier.\n",
        "        logits = self.classifier(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7k0F84rLXYL"
      },
      "source": [
        "### 3.2. Hard Parameter Sharing MTL (`HardMTL`)\n",
        "\n",
        "This model leverages the PyTorch submodule `features` defined in the base `AlexNet` class, as a shared backbone for both tasks. Then, each task has its own classification head. The forward method processes the input through the shared backbone and then through the respective task-specific heads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Y6bvHhHoLntw"
      },
      "outputs": [],
      "source": [
        "class HardMTL(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a Hard Parameter Sharing Multi-Task Learning model\n",
        "    using AlexNet as the base.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(HardMTL, self).__init__()\n",
        "        # --- Shared Feature Extractor ---\n",
        "        # We take the feature extractor directly from the base AlexNet.\n",
        "        # These weights will be updated by gradients from BOTH tasks.\n",
        "        self.features = AlexNet().features\n",
        "\n",
        "        # --- Task-Specific Classifiers ---\n",
        "        # We create two separate classifiers, one for each task.\n",
        "        # We reuse the structure of the AlexNet classifier but adjust the\n",
        "        # final output layer size for each task.\n",
        "        self.classifier_t1 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256 * 4 * 4, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, NUM_CLASSES_T1), # Output for Task 1\n",
        "        )\n",
        "        self.classifier_t2 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256 * 4 * 4, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, NUM_CLASSES_T2), # Output for Task 2\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines the forward pass for the Hard MTL model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): The input tensor.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (output_t1, output_t2)\n",
        "                   - output_t1: Logits for Task 1.\n",
        "                   - output_t2: Logits for Task 2.\n",
        "        \"\"\"\n",
        "        # Pass through the shared layers.\n",
        "        x = self.features(x)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # Pass through the task-specific heads.\n",
        "        logits_t1 = self.classifier_t1(x)\n",
        "        logits_t2 = self.classifier_t2(x)\n",
        "\n",
        "        return logits_t1, logits_t2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NR_KgYyMHyX"
      },
      "source": [
        "### 3.3. Soft Parameter Sharing (Cross-Stitch Network)\n",
        "Cross-Stitch Networks are a deep learning architectures for Multi-Task Learning (MTL). Their main idea is to let separate networks, each working on a different task, choose how to share and mix information at different layers. This happens using **cross-stitch units** that learn to combine features from both networks through linear combinations. This allows the model to adaptively decide how much information to share between tasks, rather than forcing a fixed sharing strategy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4CUhdGRQE97"
      },
      "source": [
        "**Note**: Before moving on, we need to redefine the `AlexNet` class already implemented in this notebook in order to correctly place the cross-stitch units when defining the `CrossStitchNetwork` class.\n",
        "\n",
        "---\n",
        "\n",
        "We will follow the cross-stitch network architecture described in the following paper: [*Cross-Stitch Networks for Multi-Task Learning*](https://arxiv.org/abs/1604.03539).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NLZreP_5P_VP"
      },
      "outputs": [],
      "source": [
        "class AlexNet_mod(nn.Module):\n",
        "    def __init__(self, num_outputs=10):\n",
        "        super(AlexNet_mod, self).__init__()\n",
        "\n",
        "        # Convolutional Block 1 (Conv1 -> ReLU -> MaxPool1)\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2) # Output: B, 64, 16, 16\n",
        "        )\n",
        "        # Convolutional Block 2 (Conv2 -> ReLU -> MaxPool2)\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2) # Output: B, 192, 8, 8\n",
        "        )\n",
        "        # Convolutional Block 3 (Conv3 -> ReLU -> Conv4 -> ReLU -> Conv5 -> ReLU -> MaxPool3)\n",
        "        self.conv_block3 = nn.Sequential(\n",
        "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2) # Output: B, 256, 4, 4\n",
        "        )\n",
        "\n",
        "        # Classifier Fully-Connected Block 1\n",
        "        self.fc_block1 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256 * 4 * 4, 1024),\n",
        "            nn.ReLU(inplace=True) # Output: B, 1024\n",
        "        )\n",
        "        # Classifier Fully-Connected Block 2\n",
        "        self.fc_block2 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(inplace=True) # Output: B, 1024\n",
        "        )\n",
        "        # Final Output Layer\n",
        "        self.output_layer = nn.Linear(1024, num_outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block1(x)\n",
        "        x = self.conv_block2(x)\n",
        "        x = self.conv_block3(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc_block1(x)\n",
        "        x = self.fc_block2(x)\n",
        "        logits = self.output_layer(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71QaXzHw-Fln"
      },
      "source": [
        "#### 3.3.1. Cross-Stitch Unit\n",
        "\n",
        "Cross-Stitch Unit controls the degree of information sharing between tasks. Let's break down how it works in the context of our two-task learning:\n",
        "\n",
        "- Consider a specific layer, $l$ in the network where we want to apply the Cross-Stitch unit. At this layer, we have two branches: Branch A for Task A and Branch B for Task B. Branch A provides its current feature representation, $x_A^{(l)}$, and Branch B provides its feature representation, $x_B^{(l)}$. The Cross-Stitch unit takes these two inputs and computes new, updated feature representations for each branch: $\\tilde{x}_A^{(l)}$ for Branch A and $\\tilde{x}_B^{(l)}$ for Branch B. These updated features then proceed to the subsequent layers within their respective branches.\n",
        "\n",
        "- For two-task learning, the cross-stitch unit is $2 \\times 2$ matrix, and it works as follows:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} \\tilde{x}_A^{(l)} \\\\ \\tilde{x}_B^{(l)} \\end{bmatrix} =\n",
        "\\begin{bmatrix} \\alpha_{AA}^{(l)} & \\alpha_{AB}^{(l)} \\\\ \\alpha_{BA}^{(l)} & \\alpha_{BB}^{(l)} \\end{bmatrix}\n",
        "\\begin{bmatrix} x_A^{(l)} \\\\ x_B^{(l)} \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* **$\\alpha_{AA}^{(l)}$ and $\\alpha_{BB}^{(l)}$ (Same-Task Weights):** Control how much of the original information from Task A ($x_A^{(l)}$) remains within Branch A, and similarly for Task B ($x_B^{(l)}$) within Branch B.\n",
        "* **$\\alpha_{AB}^{(l)}$ and $\\alpha_{BA}^{(l)}$ (Different-Task Weights):** Control the information exchange *between* the two tasks. Specifically, $\\alpha_{AB}^{(l)}$ determines how much of Task B's information ($x_B^{(l)}$) is incorporated into Task A's updated features ($\\tilde{x}_A^{(l)}$). Conversely, $\\alpha_{BA}^{(l)}$ controls how much of Task A's information ($x_A^{(l)}$) influences Task B's updated features ($\\tilde{x}_B^{(l)}$).\n",
        "\n",
        "The $\\alpha$ parameters can be fixed or learned during training. In this notebook, we will learn them during training, allowing the model to adaptively determine how much information to share between tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wDY0q6yNNidR"
      },
      "outputs": [],
      "source": [
        "class CrossStitchUnit(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a Cross-Stitch unit.\n",
        "    This unit learns a 2x2 matrix (alpha) to linearly combine the\n",
        "    feature maps from two task-specific networks.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initializes the Cross-Stitch unit.\n",
        "        \"\"\"\n",
        "        super(CrossStitchUnit, self).__init__()\n",
        "        # --- Learnable Alpha Matrix ---\n",
        "        # nn.Parameter tells PyTorch that this tensor should be considered\n",
        "        # a model parameter, meaning it should be tracked for gradients\n",
        "        # and updated during training by the optimizer.\n",
        "        # We use a 2x2 matrix because we have two tasks.\n",
        "        # We initialize all values to 0.5\n",
        "        self.alpha = nn.Parameter(torch.ones(2, 2) * 0.5)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        \"\"\"\n",
        "        Performs the cross-stitch operation.\n",
        "\n",
        "        Args:\n",
        "            x1 (torch.Tensor): Feature map from Task 1's network.\n",
        "            x2 (torch.Tensor): Feature map from Task 2's network.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (out1, out2)\n",
        "                   - out1: Combined feature map for Task 1.\n",
        "                   - out2: Combined feature map for Task 2.\n",
        "        \"\"\"\n",
        "        # Apply softmax to the alpha matrix to ensure the weights sum to 1.\n",
        "        alpha = torch.softmax(self.alpha, dim=1) # Softmax along the rows (dim=1)\n",
        "\n",
        "        # Perform the cross-stitch operation\n",
        "        out1 = alpha[0, 0] * x1 + alpha[0, 1] * x2\n",
        "        out2 = alpha[1, 0] * x1 + alpha[1, 1] * x2\n",
        "        return out1, out2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HstidpmRYl4G"
      },
      "source": [
        "#### 3.3.2. CrossStitchNetwork\n",
        "The architecture of the Cross-Stitch Network we will implement is based on the the following figure, which shows how the cross-stitch units are placed between the layers of the AlexNet architecture.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://github.com/Gwme13/MTL_with_Pytorch/blob/main/images/AlexNet-Cross-Stitch.PNG?raw=1\" alt=\"Cross-Stitch Network Architecture\" width=\"600\">\n",
        "</div>\n",
        "\n",
        "---\n",
        "Misra, I., Shrivastava, A., Gupta, A., & Hebert, M. (2016). *Cross-stitch Networks for Multi-task Learning*. arXiv:1604.03539. [https://arxiv.org/abs/1604.03539](https://arxiv.org/abs/1604.03539)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_gPZAuQ7UAhW"
      },
      "outputs": [],
      "source": [
        "class CrossStitchNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a Cross-Stitch Network using two modified AlexNet backbones,\n",
        "    with cross-stitch layers applied after every pooling and FC layer.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(CrossStitchNetwork, self).__init__()\n",
        "\n",
        "        self.task1_net = AlexNet_mod(num_outputs=NUM_CLASSES_T1)\n",
        "        self.task2_net = AlexNet_mod(num_outputs=NUM_CLASSES_T2)\n",
        "\n",
        "        # --- Cross-Stitch Layers ---\n",
        "        self.cs_pool1 = CrossStitchUnit()\n",
        "        self.cs_pool2 = CrossStitchUnit()\n",
        "        self.cs_pool3 = CrossStitchUnit()\n",
        "\n",
        "        self.cs_fc1 = CrossStitchUnit()\n",
        "        self.cs_fc2 = CrossStitchUnit()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines the forward pass for the Cross-Stitch Network.\n",
        "        Args:\n",
        "            x (torch.Tensor): The input tensor.\n",
        "        Returns:\n",
        "            tuple: (out1, out2) - Logits for Task 1 and Task 2.\n",
        "        \"\"\"\n",
        "        # --- Initial Input ---\n",
        "        # Both networks process the same input initially\n",
        "        x_t1 = x\n",
        "        x_t2 = x\n",
        "\n",
        "        # --- Conv Block 1 (includes Pool1) ---\n",
        "        x_t1_p1 = self.task1_net.conv_block1(x_t1)\n",
        "        x_t2_p1 = self.task2_net.conv_block1(x_t2)\n",
        "\n",
        "        x_t1_p1_cs, x_t2_p1_cs = self.cs_pool1(x_t1_p1, x_t2_p1)\n",
        "\n",
        "        # --- Conv Block 2 (includes Pool2) ---\n",
        "        x_t1_p2 = self.task1_net.conv_block2(x_t1_p1_cs)\n",
        "        x_t2_p2 = self.task2_net.conv_block2(x_t2_p1_cs)\n",
        "\n",
        "        x_t1_p2_cs, x_t2_p2_cs = self.cs_pool2(x_t1_p2, x_t2_p2)\n",
        "\n",
        "        # --- Conv Block 3 (includes Pool3) ---\n",
        "        x_t1_p3 = self.task1_net.conv_block3(x_t1_p2_cs)\n",
        "        x_t2_p3 = self.task2_net.conv_block3(x_t2_p2_cs)\n",
        "\n",
        "        x_t1_p3_cs, x_t2_p3_cs = self.cs_pool3(x_t1_p3, x_t2_p3)\n",
        "\n",
        "        # Flatten for Classifier\n",
        "        x_t1_flat = torch.flatten(x_t1_p3_cs, 1)\n",
        "        x_t2_flat = torch.flatten(x_t2_p3_cs, 1)\n",
        "\n",
        "        # --- FC Block 1 ---\n",
        "        x_t1_fc1 = self.task1_net.fc_block1(x_t1_flat)\n",
        "        x_t2_fc1 = self.task2_net.fc_block1(x_t2_flat)\n",
        "\n",
        "        x_t1_fc1_cs, x_t2_fc1_cs = self.cs_fc1(x_t1_fc1, x_t2_fc1)\n",
        "\n",
        "        # --- FC Block 2 ---\n",
        "        x_t1_fc2 = self.task1_net.fc_block2(x_t1_fc1_cs)\n",
        "        x_t2_fc2 = self.task2_net.fc_block2(x_t2_fc1_cs)\n",
        "\n",
        "        x_t1_fc2_cs, x_t2_fc2_cs = self.cs_fc2(x_t1_fc2, x_t2_fc2)\n",
        "\n",
        "        # --- Final Output Layers ---\n",
        "        logits_t1 = self.task1_net.output_layer(x_t1_fc2_cs)\n",
        "        logits_t2 = self.task2_net.output_layer(x_t2_fc2_cs)\n",
        "\n",
        "        return logits_t1, logits_t2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFcxzydGPfjJ"
      },
      "source": [
        "### 3.4. Single-Task Learning (`SingleTask`)\n",
        "\n",
        "This is simply a wrapper around the base `AlexNet` to fit into our training framework easily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "unTsY9UgPknF"
      },
      "outputs": [],
      "source": [
        "class SingleTask(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple wrapper for AlexNet to represent a Single-Task Learning model.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_outputs):\n",
        "        super(SingleTask, self).__init__()\n",
        "        self.model = AlexNet(num_outputs=num_outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkbOQMVkVmdk"
      },
      "source": [
        "## 4. Training and Evaluation Utilities\n",
        "\n",
        "We define functions to handle the training and evaluation loops. This keeps our main code cleaner and avoids repetition. We need separate functions for MTL (handling two outputs and losses) and STL (handling one).\n",
        "\n",
        "**Key PyTorch Steps in Training:**\n",
        "\n",
        "1.  `model.train()`: Sets the model to training mode (enables dropout, batch norm updates, etc.).\n",
        "2.  `optimizer.zero_grad()`: Clears old gradients before calculating new ones.\n",
        "3.  `outputs = model(inputs)`: Performs the forward pass.\n",
        "4.  `loss = criterion(outputs, labels)`: Calculates the loss.\n",
        "5.  `loss.backward()`: Computes gradients of the loss w.r.t. model parameters.\n",
        "6.  `optimizer.step()`: Updates model parameters using the computed gradients and the chosen optimization algorithm.\n",
        "\n",
        "**Key PyTorch Steps in Evaluation:**\n",
        "\n",
        "1.  `model.eval()`: Sets the model to evaluation mode (disables dropout, etc.).\n",
        "2.  `with torch.no_grad()`: Disables gradient calculation, saving memory and computation, as we don't need gradients during evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hXFCwtc7WOmV"
      },
      "outputs": [],
      "source": [
        "# Define the loss function. CrossEntropyLoss is standard for classification.\n",
        "# It combines LogSoftmax and NLLLoss in one class.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def generic_train_epoch_mtl(model, loader, optimizer, criterion, device, epoch_num):\n",
        "    \"\"\"\n",
        "    Trains an MTL model for one epoch.\n",
        "    \"\"\"\n",
        "    model.train() # Set model to training mode\n",
        "    running_total_loss = 0.0\n",
        "    running_loss_t1 = 0.0\n",
        "    running_loss_t2 = 0.0\n",
        "    running_corrects_t1 = 0\n",
        "    running_corrects_t2 = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for inputs, labels_t1, labels_t2 in loader:\n",
        "        # Move data to the selected device (GPU/CPU)\n",
        "        inputs, labels_t1, labels_t2 = inputs.to(device), labels_t1.to(device), labels_t2.to(device)\n",
        "\n",
        "        # --- Forward Pass ---\n",
        "        optimizer.zero_grad() # Zero the parameter gradients\n",
        "        outputs_t1, outputs_t2 = model(inputs) # Get model outputs\n",
        "\n",
        "        # --- Loss Calculation ---\n",
        "        loss_t1 = criterion(outputs_t1, labels_t1)\n",
        "        loss_t2 = criterion(outputs_t2, labels_t2)\n",
        "        # Simple MTL loss: Sum of individual task losses.\n",
        "        # More advanced methods might weigh these differently.\n",
        "        total_loss = loss_t1 + loss_t2\n",
        "\n",
        "        # --- Backward Pass & Optimization ---\n",
        "        total_loss.backward() # Compute gradients\n",
        "        optimizer.step() # Update weights\n",
        "\n",
        "        # --- Statistics ---\n",
        "        _, preds_t1 = torch.max(outputs_t1, 1) # Get predicted class index (highest logit)\n",
        "        _, preds_t2 = torch.max(outputs_t2, 1)\n",
        "\n",
        "        running_total_loss += total_loss.item() * images.size(0)\n",
        "        running_loss_t1 += loss_t1.item() * inputs.size(0)\n",
        "        running_loss_t2 += loss_t2.item() * inputs.size(0)\n",
        "\n",
        "        running_corrects_t1 += torch.sum(preds_t1 == labels_t1.data)\n",
        "        running_corrects_t2 += torch.sum(preds_t2 == labels_t2.data)\n",
        "\n",
        "        total_samples += inputs.size(0)\n",
        "\n",
        "    epoch_total_loss = running_total_loss / total_samples\n",
        "    epoch_loss_t1 = running_loss_t1 / total_samples\n",
        "    epoch_acc_t1 = running_corrects_t1.double() / total_samples\n",
        "    epoch_loss_t2 = running_loss_t2 / total_samples\n",
        "    epoch_acc_t2 = running_corrects_t2.double() / total_samples\n",
        "\n",
        "    print(f\"Training, Epoch {epoch_num}:\\n- Loss: {epoch_total_loss:.4f}\\n- T1: Loss: {epoch_loss_t1:.4f} - Acc: {epoch_acc_t1*100:.2f}%\\n\"\n",
        "          f\"- T2: Loss: {epoch_loss_t2:.4f} - Acc: {epoch_acc_t2*100:.2f}%\")\n",
        "\n",
        "    return epoch_total_loss, epoch_loss_t1, epoch_acc_t1.item(), epoch_loss_t2, epoch_acc_t2.item()\n",
        "\n",
        "\n",
        "def generic_evaluate_epoch_mtl(model, loader, criterion, device, epoch_num, phase=\"Validation\"):\n",
        "    \"\"\"\n",
        "    Evaluates an MTL model for one epoch.\n",
        "    \"\"\"\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    running_total_loss = 0.0\n",
        "    running_loss_t1 = 0.0\n",
        "    running_loss_t2 = 0.0\n",
        "    running_corrects_t1 = 0\n",
        "    running_corrects_t2 = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculations\n",
        "        for inputs, labels_t1, labels_t2 in loader:\n",
        "            inputs, labels_t1, labels_t2 = inputs.to(device), labels_t1.to(device), labels_t2.to(device)\n",
        "\n",
        "            outputs_t1, outputs_t2 = model(inputs)\n",
        "            loss_t1 = criterion(outputs_t1, labels_t1)\n",
        "            loss_t2 = criterion(outputs_t2, labels_t2)\n",
        "\n",
        "            total_loss = loss_t1 + loss_t2\n",
        "\n",
        "            _, preds_t1 = torch.max(outputs_t1, 1)\n",
        "            _, preds_t2 = torch.max(outputs_t2, 1)\n",
        "\n",
        "            running_total_loss += total_loss.item() * images.size(0)\n",
        "            running_loss_t1 += loss_t1.item() * inputs.size(0)\n",
        "            running_loss_t2 += loss_t2.item() * inputs.size(0)\n",
        "\n",
        "            running_corrects_t1 += torch.sum(preds_t1 == labels_t1.data)\n",
        "            running_corrects_t2 += torch.sum(preds_t2 == labels_t2.data)\n",
        "\n",
        "            total_samples += inputs.size(0)\n",
        "\n",
        "    epoch_total_loss = running_total_loss / total_samples\n",
        "    epoch_loss_t1 = running_loss_t1 / total_samples\n",
        "    epoch_acc_t1 = running_corrects_t1.double() / total_samples\n",
        "    epoch_loss_t2 = running_loss_t2 / total_samples\n",
        "    epoch_acc_t2 = running_corrects_t2.double() / total_samples\n",
        "\n",
        "    print(f\"{phase}{f', Epoch {epoch_num}:' if epoch_num else ''}\\n- Loss: {epoch_total_loss:.4f}\\n- T1: Loss: {epoch_loss_t1:.4f} - Acc: {epoch_acc_t1*100:.2f}%\\n\"\n",
        "          f\"- T2: Loss: {epoch_loss_t2:.4f} - Acc: {epoch_acc_t2*100:.2f}%\")\n",
        "\n",
        "    return epoch_total_loss, epoch_loss_t1, epoch_acc_t1.item(), epoch_loss_t2, epoch_acc_t2.item()\n",
        "\n",
        "\n",
        "# --- Functions for STL ---\n",
        "# These are similar but simpler as they only handle one task.\n",
        "def generic_train_epoch_stl(model, loader, optimizer, criterion, device, task_id, epoch_num):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for inputs, labels_t1, labels_t2 in loader:\n",
        "        labels = labels_t1 if task_id == \"task1\" else labels_t2\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "        total_samples += inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = running_corrects.double() / total_samples\n",
        "\n",
        "    print(f\"Training, Epoch {epoch_num}:\\n- Loss: {epoch_loss:.4f} - Acc: {epoch_acc*100:.2f}%\")\n",
        "    return epoch_loss, epoch_acc.item()\n",
        "\n",
        "\n",
        "def generic_evaluate_epoch_stl(model, loader, criterion, device, task_id, epoch_num, phase=\"Validation\"):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels_t1, labels_t2 in loader:\n",
        "            labels = labels_t1 if task_id == \"task1\" else labels_t2\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            total_samples += inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total_samples\n",
        "    epoch_acc = running_corrects.double() / total_samples\n",
        "\n",
        "    print(f\"{phase}{f', Epoch {epoch_num}: ' if epoch_num else ''}\\n- Loss: {epoch_loss:.4f} - Acc: {epoch_acc*100:.2f}%\")\n",
        "    return epoch_loss, epoch_acc.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rvzcqOPtbGwd"
      },
      "outputs": [],
      "source": [
        "# --- Generic Training Loop ---\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs, model_type, save_path, task_id=None, alpha_history_list=None):\n",
        "    \"\"\"\n",
        "    A generic function to train either MTL or STL models.\n",
        "    \"\"\"\n",
        "    best_validation_loss = float('inf')\n",
        "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
        "    if model_type == 'mtl':\n",
        "        history = {'train_loss': [], 'val_loss': [], 'train_loss_t1': [], 'train_acc_t1': [], 'val_loss_t1': [], 'val_acc_t1': [],\n",
        "                   'train_loss_t2': [], 'train_acc_t2': [], 'val_loss_t2': [], 'val_acc_t2': []}\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        if model_type == 'mtl':\n",
        "            train_loss, train_loss_t1, train_acc_t1, train_loss_t2, train_acc_t2 = generic_train_epoch_mtl(\n",
        "                model, train_loader, optimizer, criterion, device, epoch\n",
        "            )\n",
        "            val_loss, val_loss_t1, val_acc_t1, val_loss_t2, val_acc_t2 = generic_evaluate_epoch_mtl(\n",
        "                model, val_loader, criterion, device, epoch\n",
        "            )\n",
        "\n",
        "            history['train_loss_t1'].append(train_loss_t1)\n",
        "            history['train_acc_t1'].append(train_acc_t1)\n",
        "            history['val_loss_t1'].append(val_loss_t1)\n",
        "            history['val_acc_t1'].append(val_acc_t1)\n",
        "            history['train_loss_t2'].append(train_loss_t2)\n",
        "            history['train_acc_t2'].append(train_acc_t2)\n",
        "            history['val_loss_t2'].append(val_loss_t2)\n",
        "            history['val_acc_t2'].append(val_acc_t2)\n",
        "\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['val_loss'].append(val_loss)\n",
        "\n",
        "            current_validation_loss = val_loss\n",
        "\n",
        "        else: # STL\n",
        "            train_loss, train_acc = generic_train_epoch_stl(\n",
        "                model, train_loader, optimizer, criterion, device, task_id, epoch\n",
        "            )\n",
        "            val_loss, val_acc = generic_evaluate_epoch_stl(\n",
        "                model, val_loader, criterion, device, task_id, epoch\n",
        "            )\n",
        "\n",
        "            history['train_loss'].append(train_loss)\n",
        "            history['train_acc'].append(train_acc)\n",
        "            history['val_loss'].append(val_loss)\n",
        "            history['val_acc'].append(val_acc)\n",
        "\n",
        "            current_validation_loss = val_loss\n",
        "\n",
        "        # Save alpha values if training Cross-Stitch\n",
        "        if alpha_history_list is not None and isinstance(model, CrossStitchNetwork):\n",
        "            alpha_history_list[0].append(model.cs_pool1.alpha.data.cpu().numpy().copy())\n",
        "            alpha_history_list[1].append(model.cs_pool2.alpha.data.cpu().numpy().copy())\n",
        "            alpha_history_list[2].append(model.cs_pool3.alpha.data.cpu().numpy().copy())\n",
        "\n",
        "            alpha_history_list[3].append(model.cs_fc1.alpha.data.cpu().numpy().copy())\n",
        "            alpha_history_list[4].append(model.cs_fc2.alpha.data.cpu().numpy().copy())\n",
        "\n",
        "        # Save the best model based on validation loss\n",
        "        if current_validation_loss < best_validation_loss:\n",
        "            best_validation_loss = current_validation_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeP58lx_bWqL"
      },
      "source": [
        "## 4. Training The Models\n",
        "\n",
        "Now we put everything together and train each of our models: Hard MTL, Cross-Stitch MTL, and the two STL models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHn0sMSFgQZO"
      },
      "source": [
        "### 4.1. Hard Parameter Sharing MTL Training\n",
        "\n",
        "We instantiate the `HardMTL` model, move it to the configured device, set up the Adam optimizer, and start the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "TvYJm9e_blxX",
        "outputId": "755882d6-0fc4-4300-811d-97fe60ec8aa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training Hard Parameter Sharing MTL ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-5253257d5820>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer_hard_mtl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_hard_mtl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m history_hard_mtl = train_model(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel_hard_mtl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_hard_mtl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mtl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'best_alexnet_hard_mtl_model.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-d2dee67520b8>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, device, num_epochs, model_type, save_path, task_id, alpha_history_list)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mtl'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             train_loss, train_loss_t1, train_acc_t1, train_loss_t2, train_acc_t2 = generic_train_epoch_mtl(\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             )\n",
            "\u001b[0;32m<ipython-input-15-9f65308b7ae9>\u001b[0m in \u001b[0;36mgeneric_train_epoch_mtl\u001b[0;34m(model, loader, optimizer, criterion, device, epoch_num)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_t1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_t2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Move data to the selected device (GPU/CPU)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_t1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_t2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_t1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_t2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-d45ffb57bffb>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Apply any final transforms (ToTensor, Normalize, etc.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Rotation label is simply angle_idx (0..3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_num_channels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m     \u001b[0;31m# put it from HWC to CHW format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(\"--- Training Hard Parameter Sharing MTL ---\")\n",
        "model_hard_mtl = HardMTL().to(device)\n",
        "optimizer_hard_mtl = optim.Adam(model_hard_mtl.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "history_hard_mtl = train_model(\n",
        "    model_hard_mtl, train_loader, test_loader, optimizer_hard_mtl, criterion,\n",
        "    device, NUM_EPOCHS, 'mtl', 'best_alexnet_hard_mtl_model.pt'\n",
        ")\n",
        "print(\"--- Hard MTL Training Finished ---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIpXB4dxgMND"
      },
      "source": [
        "### 4.2. Cross-Stitch Network (Soft Sharing) MTL Training\n",
        "\n",
        "Here, we train the `CrossStitchNetwork`. We also initialize lists to store the `alpha` coefficients from each Cross-Stitch unit during training. This will allow us to visualize how the network learns to share information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_bjYqDFgibb"
      },
      "outputs": [],
      "source": [
        "print(\"--- Training Cross-Stitch Network MTL ---\")\n",
        "model_cs = CrossStitchNetwork().to(device)\n",
        "optimizer_cs = optim.Adam(model_cs.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Lists to store alpha values (one list per CS layer)\n",
        "alphas_cs1_history = []\n",
        "alphas_cs2_history = []\n",
        "alphas_cs3_history = []\n",
        "alphas_cs4_history = []\n",
        "alphas_cs5_history = []\n",
        "alpha_history_list_cs = [alphas_cs1_history, alphas_cs2_history, alphas_cs3_history, alphas_cs4_history, alphas_cs5_history]\n",
        "\n",
        "history_cs = train_model(\n",
        "    model_cs, train_loader, test_loader, optimizer_cs, criterion,\n",
        "    device, NUM_EPOCHS, 'mtl', 'best_alexnet_cs_model.pt',\n",
        "    alpha_history_list=alpha_history_list_cs\n",
        ")\n",
        "print(\"--- Cross-Stitch Training Finished ---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP9KSbpzma4d"
      },
      "source": [
        "### 4.3. Single-Task Learning (STL) Training\n",
        "\n",
        "We train two separate models, one for each task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezBDg0v9mgsv"
      },
      "source": [
        "#### 4.3.1. STL Task 1 (CIFAR-10 Classification)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YB8E7HyfpJvb"
      },
      "outputs": [],
      "source": [
        "print(\"--- Training STL Task 1 ---\")\n",
        "model_stl_task1 = SingleTask(num_outputs=NUM_CLASSES_T1).to(device)\n",
        "optimizer_stl_task1 = optim.Adam(model_stl_task1.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "history_stl_task1 = train_model(\n",
        "    model_stl_task1, train_loader, test_loader, optimizer_stl_task1, criterion,\n",
        "    device, NUM_EPOCHS, 'stl', 'best_alexnet_stl_task1_model.pt', task_id='task1'\n",
        ")\n",
        "print(\"--- STL Task 1 Training Finished ---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfqpoZMypOfE"
      },
      "source": [
        "#### 4.3.2. STL Task 2 (Rotation Classification)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2VuvClOpR04"
      },
      "outputs": [],
      "source": [
        "print(\"--- Training STL Task 2 ---\")\n",
        "model_stl_task2 = SingleTask(num_outputs=NUM_CLASSES_T2).to(device)\n",
        "optimizer_stl_task2 = optim.Adam(model_stl_task2.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "history_stl_task2 = train_model(\n",
        "    model_stl_task2, train_loader, test_loader, optimizer_stl_task2, criterion,\n",
        "    device, NUM_EPOCHS, 'stl', 'best_alexnet_stl_task2_model.pt', task_id='task2'\n",
        ")\n",
        "print(\"--- STL Task 2 Training Finished ---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXt5MuDVpWXi"
      },
      "source": [
        "## 5. Results and Analysis\n",
        "\n",
        "Now that our models are trained, we will analyze their performance. Specifically, we will:\n",
        "\n",
        "1.  Plot training and validation curves (loss and accuracy).\n",
        "2.  Plot the evolution of the alpha coefficients of the Cross-Stitch Network during training.\n",
        "3.  Evaluate models on the test set for a final comparison.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkRZkqgapmgE"
      },
      "source": [
        "### 5.1. Learning Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_OZRYs5p1nR"
      },
      "outputs": [],
      "source": [
        "def plot_history(history, title, model_type):\n",
        "    \"\"\"Plots the training and validation history with accuracy in percentage (2 decimal places).\"\"\"\n",
        "    epochs_range = range(1, NUM_EPOCHS + 1)\n",
        "\n",
        "    formatter = mtick.FuncFormatter(lambda y, pos: f'{y:.2f}%')\n",
        "\n",
        "    if model_type == 'mtl':\n",
        "        plt.figure(figsize=(14, 10))\n",
        "        plt.suptitle(title, fontsize=16, y=1.02)\n",
        "\n",
        "        # Task 1 - Loss\n",
        "        plt.subplot(2, 2, 1)\n",
        "        plt.plot(epochs_range, history['train_loss_t1'], 'b-', label='Train Loss T1')\n",
        "        plt.plot(epochs_range, history['val_loss_t1'], 'b--', label='Val Loss T1')\n",
        "        plt.title('Task 1 - Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Task 1 - Accuracy (%)\n",
        "        plt.subplot(2, 2, 2)\n",
        "        plt.plot(epochs_range, [acc * 100 for acc in history['train_acc_t1']], 'g-', label='Train Acc T1')\n",
        "        plt.plot(epochs_range, [acc * 100 for acc in history['val_acc_t1']], 'g--', label='Val Acc T1')\n",
        "        plt.title('Task 1 - Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy (%)')\n",
        "        plt.gca().yaxis.set_major_formatter(formatter)\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Task 2 - Loss\n",
        "        plt.subplot(2, 2, 3)\n",
        "        plt.plot(epochs_range, history['train_loss_t2'], 'r-', label='Train Loss T2')\n",
        "        plt.plot(epochs_range, history['val_loss_t2'], 'r--', label='Val Loss T2')\n",
        "        plt.title('Task 2 - Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Task 2 - Accuracy (%)\n",
        "        plt.subplot(2, 2, 4)\n",
        "        plt.plot(epochs_range, [acc * 100 for acc in history['train_acc_t2']], 'm-', label='Train Acc T2')\n",
        "        plt.plot(epochs_range, [acc * 100 for acc in history['val_acc_t2']], 'm--', label='Val Acc T2')\n",
        "        plt.title('Task 2 - Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy (%)')\n",
        "        plt.gca().yaxis.set_major_formatter(formatter)\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
        "        plt.show()\n",
        "\n",
        "        # Overall Loss\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.plot(epochs_range, history['train_loss'], 'k-', label='Overall Train Loss')\n",
        "        plt.plot(epochs_range, history['val_loss'], 'k--', label='Overall Val Loss')\n",
        "        plt.title(f'{title} - Overall Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    else: # STL\n",
        "        plt.figure(figsize=(14, 5))\n",
        "        plt.suptitle(title, fontsize=16, y=1.02)\n",
        "\n",
        "        # Loss\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(epochs_range, history['train_loss'], 'b-', label='Train Loss')\n",
        "        plt.plot(epochs_range, history['val_loss'], 'b--', label='Val Loss')\n",
        "        plt.title('Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # Accuracy (%)\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(epochs_range, [acc * 100 for acc in history['train_acc']], 'g-', label='Train Acc')\n",
        "        plt.plot(epochs_range, [acc * 100 for acc in history['val_acc']], 'g--', label='Val Acc')\n",
        "        plt.title('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy (%)')\n",
        "        plt.gca().yaxis.set_major_formatter(formatter)\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "        plt.show()\n",
        "\n",
        "plot_history(history_hard_mtl, \"Hard-Parameter Sharing Learning Curves\", \"mtl\")\n",
        "plot_history(history_cs, \"Cross-Stitch Network Learning Curves\", \"mtl\")\n",
        "plot_history(history_stl_task1, \"STL on Task 1 Learning Curves\", \"stl\")\n",
        "plot_history(history_stl_task2, \"STL on Task 2 Learning Curves\", \"stl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecf_qGZZqDgV"
      },
      "source": [
        "### 5.2. Alpha Coefficients Evolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1Z6BVzfqdjT"
      },
      "outputs": [],
      "source": [
        "def plot_alpha_history(\n",
        "    alpha_history_cs1,\n",
        "    alpha_history_cs2,\n",
        "    alpha_history_cs3,\n",
        "    alpha_history_cs4,\n",
        "    alpha_history_cs5,\n",
        "    num_epochs):\n",
        "\n",
        "    \"\"\"Plots the evolution of alpha coefficients.\"\"\"\n",
        "    alphas_cs1_np = np.array(alpha_history_cs1)\n",
        "    alphas_cs2_np = np.array(alpha_history_cs2)\n",
        "    alphas_cs3_np = np.array(alpha_history_cs3)\n",
        "    alphas_cs4_np = np.array(alpha_history_cs4)\n",
        "    alphas_cs5_np = np.array(alpha_history_cs5)\n",
        "    epochs_range = range(1, num_epochs + 1)\n",
        "\n",
        "    # Plotting alpha values after pooling layer 1\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.suptitle(\"Cross-Stitch weights after 1st pooling layer\", fontsize=16)\n",
        "    labels = [r'$\\alpha_{11}$ (T1<-T1)', r'$\\alpha_{12}$ (T1<-T2)', r'$\\alpha_{21}$ (T2<-T1)', r'$\\alpha_{22}$ (T2<-T2)']\n",
        "    colors = ['blue', 'orange', 'green', 'red']\n",
        "    indices = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
        "\n",
        "    for i in range(4):\n",
        "        plt.subplot(2, 2, i + 1)\n",
        "        plt.plot(epochs_range, alphas_cs1_np[:, indices[i][0], indices[i][1]], label=labels[i], color=colors[i])\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Weight\")\n",
        "        plt.title(labels[i])\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.ylim(-0.1, 1.1)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "    # Plotting alpha values after pooling layer 2\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.suptitle(\"Cross-Stitch weights after 2nd pooling layer\", fontsize=16)\n",
        "\n",
        "    for i in range(4):\n",
        "        plt.subplot(2, 2, i + 1)\n",
        "        plt.plot(epochs_range, alphas_cs2_np[:, indices[i][0], indices[i][1]], label=labels[i], color=colors[i])\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Weight\")\n",
        "        plt.title(labels[i])\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.ylim(-0.1, 1.1)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "    # Plotting alpha values after pooling layer 3\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.suptitle(\"Cross-Stitch weights after 3th pooling layer\", fontsize=16)\n",
        "\n",
        "    for i in range(4):\n",
        "        plt.subplot(2, 2, i + 1)\n",
        "        plt.plot(epochs_range, alphas_cs3_np[:, indices[i][0], indices[i][1]], label=labels[i], color=colors[i])\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Weight\")\n",
        "        plt.title(labels[i])\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.ylim(-0.1, 1.1)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "    # Plotting alpha values after fully connected layer 1\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.suptitle(\"Cross-Stitch weights after 1st fully connected layer\", fontsize=16)\n",
        "\n",
        "    for i in range(4):\n",
        "        plt.subplot(2, 2, i + 1)\n",
        "        plt.plot(epochs_range, alphas_cs4_np[:, indices[i][0], indices[i][1]], label=labels[i], color=colors[i])\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Weight\")\n",
        "        plt.title(labels[i])\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.ylim(-0.1, 1.1)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "    # Plotting alpha values after fully connected layer 2\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.suptitle(\"Cross-Stitch weights after 2nd fully connected layerr\", fontsize=16)\n",
        "\n",
        "    for i in range(4):\n",
        "        plt.subplot(2, 2, i + 1)\n",
        "        plt.plot(epochs_range, alphas_cs5_np[:, indices[i][0], indices[i][1]], label=labels[i], color=colors[i])\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Weight\")\n",
        "        plt.title(labels[i])\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.ylim(-0.1, 1.1)\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "    plt.show()\n",
        "\n",
        "plot_alpha_history(\n",
        "    alpha_history_list_cs[0],\n",
        "    alpha_history_list_cs[1],\n",
        "    alpha_history_list_cs[2],\n",
        "    alpha_history_list_cs[3],\n",
        "    alpha_history_list_cs[4],\n",
        "    NUM_EPOCHS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hF6QkLCeKvhs"
      },
      "outputs": [],
      "source": [
        "# Count the number of parameters in each model\n",
        "def count_parameters(model):\n",
        "    \"\"\"\n",
        "    Counts the number of trainable parameters in a PyTorch model.\n",
        "    \"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# Print Summary Table\n",
        "print(f\"| Approach                  | Trainable Parameters                  |\")\n",
        "print(f\"|-------------------------- |---------------------------------------|\")\n",
        "print(f\"| Hard-Parameter sharing    | {count_parameters(model_hard_mtl):,}  \")\n",
        "print(f\"| Cross-Stitch Network      | {count_parameters(model_cs):,}        \")\n",
        "print(f\"| STL on Task 1             | {count_parameters(model_stl_task1):,} \")\n",
        "print(f\"| STL on Task 2             | {count_parameters(model_stl_task2):,} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD7G9ekAqsQt"
      },
      "source": [
        "### 5.3. Final Evaluation on Test Set\n",
        "We load the *best* saved version of each model (based on validation performance during training) and evaluate it on the test set. This gives us our final comparison metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTnu7RExrAxw"
      },
      "outputs": [],
      "source": [
        "print(\"\\n--- Final Evaluation on Test Set ---\\n\")\n",
        "\n",
        "print(\"\\nHard-Parameter sharing:\")\n",
        "# --- Load and Evaluate Hard MTL ---\n",
        "model_hard_mtl.load_state_dict(torch.load('best_alexnet_hard_mtl_model.pt', map_location=device))\n",
        "_, test_loss_t1_h, test_acc_t1_h, test_loss_t2_h, test_acc_t2_h = generic_evaluate_epoch_mtl(\n",
        "    model_hard_mtl, test_loader, criterion, device, None, phase=\"Test\"\n",
        ")\n",
        "\n",
        "print(\"\\nCross-Stitch Network:\\n\")\n",
        "# --- Load and Evaluate Cross-Stitch ---\n",
        "model_cs.load_state_dict(torch.load('best_alexnet_cs_model.pt', map_location=device))\n",
        "_, test_loss_t1_c, test_acc_t1_c, test_loss_t2_c, test_acc_t2_c = generic_evaluate_epoch_mtl(\n",
        "    model_cs, test_loader, criterion, device, None, phase=\"Test\"\n",
        ")\n",
        "\n",
        "print(\"\\nSTL on Task1:\\n\")\n",
        "# --- Load and Evaluate STL Task 1 ---\n",
        "model_stl_task1.load_state_dict(torch.load('best_alexnet_stl_task1_model.pt', map_location=device))\n",
        "test_loss_t1_s, test_acc_t1_s = generic_evaluate_epoch_stl(\n",
        "    model_stl_task1, test_loader, criterion, device, \"task1\", None, phase=\"Test\"\n",
        ")\n",
        "\n",
        "print(\"\\nSTL on Task2:\\n\")\n",
        "# --- Load and Evaluate STL Task 2 ---\n",
        "model_stl_task2.load_state_dict(torch.load('best_alexnet_stl_task2_model.pt', map_location=device))\n",
        "test_loss_t2_s, test_acc_t2_s = generic_evaluate_epoch_stl(\n",
        "    model_stl_task2, test_loader, criterion, device, \"task2\", None, phase=\"Test\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idrRWlYr-Flr"
      },
      "source": [
        "## Conclusions\n",
        "\n",
        "This didactic exploration aimed to understand the mechanisms and behaviors of Single-Task Learning (STL) versus Multi-Task Learning (MTL) paradigms. The experiments were conducted on the CIFAR-10 dataset, using an AlexNet architecture, with a focus on comparing different MTL strategies against STL baselines for two derived tasks: Task 1 (10-class object classification) and Task 2 (4-class rotation classification).\n",
        "\n",
        "### Summary of Key Quantitative Findings from the Latest Execution\n",
        "\n",
        "Based on the analysis of the final evaluation results on the test set, the following observations were made regarding model performance:\n",
        "\n",
        "* **Performance on Task 1 (10-class Object Classification):**\n",
        "    * The STL model dedicated to T1 achieved an accuracy of **75.30%**.\n",
        "    * The MTL models demonstrated slightly improved performance:\n",
        "        * **Hard-Parameter Sharing** achieved **76.66% accuracy**.\n",
        "        * **Cross-Stitch Network** achieved **79.06% accuracy**, marking the best performance for T1 among the tested models.\n",
        "\n",
        "* **Performance on Task 2 (4-class Rotation Classification):**\n",
        "    * The STL model dedicated to T2 obtained an accuracy of **79.51%**.\n",
        "    * For this task, the MTL models also showed improvements:\n",
        "        * **Hard-Parameter Sharing** achieved **82.94% accuracy**.\n",
        "        * **Cross-Stitch Network** achieved **84.33% accuracy**, again representing the top performance for T2 among the tested models.\n",
        "\n",
        "### Analysis of Learning Curves and Cross-Stitch Network Behavior\n",
        "\n",
        "Further analysis of the training process and the internal dynamics of the Cross-Stitch Network provided qualitative insights:\n",
        "\n",
        "* **A. Learning Curve Observations:**\n",
        "    Looking at the learning curves revealed overfitting tendencies across all models. While MTL did not eliminate this issue, a notable observation is that STL models tended to show signs of overfitting earlier in the training process compared to the MTL models, including the Cross-Stitch network. This suggests that MTL might help in deferring the onset of overfitting, even if it does not fully resolve it.\n",
        "\n",
        "* **B. Cross-Stitch Network: $\\alpha$ Coefficient Dynamics:**\n",
        "    The $\\alpha$ coefficients, which control the degree of feature sharing between task-specific branches in the Cross-Stitch Network, showed distinct patterns across layers:\n",
        "    1.  **Post-Pooling Layers (Early Convolutional Stages):** In the early network layers, the branch for Task 1 (object classification) heavily utilized features from the branch for Task 2 (rotation classification). This indicates that Task 1 benefits from the more general, broad features that Task 2 identifies, which likely capture basic visual primitives useful for both tasks.\n",
        "    2.  **Post-Fully Connected Layers (Later Classification Stages):** The information sharing strategy shifted significantly in the deeper, classification-specific layers. Initially, both task branches utilized a mixture of features from each other. However, as training progressed and features became more task-specialized, the Task 1 branch increasingly relied on its own developed features. Conversely, the Task 2 branch began to leverage the specialized features developed by the Task 1 branch, reducing its reliance on its own.\n",
        "\n",
        "### Final Considerations\n",
        "\n",
        "The combined quantitative and qualitative results provide interesting, but not satisfactory, insights into MTL's efficacy in this specific setup. While the final test results show that MTL approaches, particularly the Cross-Stitch Network, perform slightly better than STL, the improvements are modest. The Cross-Stitch Network achieved the highest accuracy for both tasks, suggesting that adaptive sharing of features can yield some benefits even when tasks are not strongly correlated. Task 1 necessitates learning specific, detailed features for object discrimination, whereas Task 2 likely depends on more global, structural features related to image orientation.\n",
        "\n",
        "The behavior of the $\\alpha$ values in the Cross-Stitch Network offers a plausible hypothesis for how tasks *might* assist each other, potentially contributing to the slight advantages observed:\n",
        "* In the **early network layers**, Task 1's reliance on Task 2's features suggests that Task 2, by focusing on rotation, could encourage the learning of somewhat more robust or orientation-aware low-level features. These foundational features might offer a slightly better starting point for Task 1.\n",
        "* In the **later, fully connected layers**, the dynamic shift where Task 1 focuses on its own specialized features while Task 2 begins to leverage Task 1's features is intriguing. We can argue that knowing *what* an object is (features from Task 1) could aid in determining its rotation (Task 2).\n",
        "\n",
        "This suggests that even with moderately correlated tasks, an adaptive sharing mechanism like Cross-Stitch can find ways to combine learned knowledge. While this may act as a form of regularization, leading to the observed delay in overfitting and marginal gains in test accuracy, the overall benefit in terms of performance uplift remains limited in this experimental context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XJQ01-M-Flr"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}